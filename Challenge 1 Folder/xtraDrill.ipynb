{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-21T13:52:42.342821Z",
     "iopub.status.busy": "2024-12-21T13:52:42.342496Z",
     "iopub.status.idle": "2024-12-21T13:52:48.640468Z",
     "shell.execute_reply": "2024-12-21T13:52:48.639353Z",
     "shell.execute_reply.started": "2024-12-21T13:52:42.342799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Memory Management\n",
    "import torch\n",
    "import gc\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T13:52:59.004895Z",
     "iopub.status.busy": "2024-12-21T13:52:59.003953Z",
     "iopub.status.idle": "2024-12-21T13:52:59.009937Z",
     "shell.execute_reply": "2024-12-21T13:52:59.008958Z",
     "shell.execute_reply.started": "2024-12-21T13:52:59.004849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: Constants with minimal memory usage\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 4  # Very small batch size\n",
    "MAX_LENGTH = 64  # Reduced sequence length\n",
    "LEARNING_RATE = 1e-4  # Slightly adjusted\n",
    "MODEL_NAME = \"google/mt5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T13:53:12.919036Z",
     "iopub.status.busy": "2024-12-21T13:53:12.918633Z",
     "iopub.status.idle": "2024-12-21T13:53:14.435822Z",
     "shell.execute_reply": "2024-12-21T13:53:14.434875Z",
     "shell.execute_reply.started": "2024-12-21T13:53:12.919004Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4505\n",
      "Validation samples: 501\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Dataset with Memory Optimization\n",
    "def load_and_prepare_data():\n",
    "    dataset = load_dataset(\"SKNahin/bengali-transliteration-data\")\n",
    "    train_data = dataset[\"train\"]\n",
    "    \n",
    "    # Convert to lists and clear dataset from memory\n",
    "    banglish_texts = list(train_data[\"rm\"])\n",
    "    bengali_texts = list(train_data[\"bn\"])\n",
    "    del dataset, train_data\n",
    "    gc.collect()\n",
    "    \n",
    "    return train_test_split(\n",
    "        banglish_texts, bengali_texts, \n",
    "        test_size=0.1, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "train_banglish, val_banglish, train_bengali, val_bengali = load_and_prepare_data()\n",
    "print(f\"Training samples: {len(train_banglish)}\")\n",
    "print(f\"Validation samples: {len(val_banglish)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T13:53:27.706850Z",
     "iopub.status.busy": "2024-12-21T13:53:27.706530Z",
     "iopub.status.idle": "2024-12-21T13:53:27.712921Z",
     "shell.execute_reply": "2024-12-21T13:53:27.711977Z",
     "shell.execute_reply.started": "2024-12-21T13:53:27.706823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Memory-Efficient Dataset Class\n",
    "class BanglishDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        banglish_text = \"transliterate: \" + str(self.texts[idx])\n",
    "        bengali_text = str(self.labels[idx])\n",
    "        \n",
    "        # Tokenize with smaller max_length\n",
    "        source = self.tokenizer(\n",
    "            banglish_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target = self.tokenizer(\n",
    "            bengali_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": source[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": source[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": target[\"input_ids\"].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T13:56:45.569975Z",
     "iopub.status.busy": "2024-12-21T13:56:45.569657Z",
     "iopub.status.idle": "2024-12-21T13:56:49.765195Z",
     "shell.execute_reply": "2024-12-21T13:56:49.764450Z",
     "shell.execute_reply.started": "2024-12-21T13:56:45.569950Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Initialize Model with Memory Optimizations (Updated)\n",
    "tokenizer = MT5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load model with updated config\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = BanglishDataset(train_banglish, train_bengali, tokenizer, MAX_LENGTH)\n",
    "val_dataset = BanglishDataset(val_banglish, val_bengali, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T13:57:23.155259Z",
     "iopub.status.busy": "2024-12-21T13:57:23.154888Z",
     "iopub.status.idle": "2024-12-21T13:57:23.189449Z",
     "shell.execute_reply": "2024-12-21T13:57:23.188841Z",
     "shell.execute_reply.started": "2024-12-21T13:57:23.155230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: Memory-Optimized Training Arguments\n",
    "# Cell 6: Memory-Optimized Training Arguments (Updated)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=400,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    # Disable fp16 and use bf16 instead if available\n",
    "    fp16=False,\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_num_workers=0,\n",
    "    optim=\"adamw_torch\"\n",
    ")\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T13:57:43.145873Z",
     "iopub.status.busy": "2024-12-21T13:57:43.145552Z",
     "iopub.status.idle": "2024-12-21T14:08:17.830793Z",
     "shell.execute_reply": "2024-12-21T14:08:17.830123Z",
     "shell.execute_reply.started": "2024-12-21T13:57:43.145848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='843' max='843' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [843/843 10:33, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Training with Error Handling (Updated)\n",
    "def train_with_error_handling():\n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=DataCollatorForSeq2Seq(\n",
    "            tokenizer,\n",
    "            model=model,\n",
    "            padding=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Clear memory before training\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {str(e)}\")\n",
    "        # Print more detailed error information\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return False\n",
    "\n",
    "# Start training\n",
    "success = train_with_error_handling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T14:09:42.025809Z",
     "iopub.status.busy": "2024-12-21T14:09:42.025440Z",
     "iopub.status.idle": "2024-12-21T14:09:43.021417Z",
     "shell.execute_reply": "2024-12-21T14:09:43.020525Z",
     "shell.execute_reply.started": "2024-12-21T14:09:42.025779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 8: Improved Test Function\n",
    "def translate_banglish(text):\n",
    "    # Prepare input\n",
    "    inputs = tokenizer(\"transliterate: \" + text, \n",
    "                      return_tensors=\"pt\", \n",
    "                      padding=True, \n",
    "                      truncation=True, \n",
    "                      max_length=64)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate translation with better parameters\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=64,\n",
    "        num_beams=5,  # Beam search for better results\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=False,  # Deterministic generation\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode properly\n",
    "    translated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated.strip()\n",
    "\n",
    "# Test with more examples\n",
    "test_sentences = [\n",
    "    \"ami tomake bhalobashi\",\n",
    "    \"kemon acho\",\n",
    "    \"bangla bhasha amader praner bhasha\",\n",
    "    \"amar naam\",\n",
    "    \"tumi kothay thako\"\n",
    "]\n",
    "\n",
    "print(\"\\nTest Translations:\")\n",
    "for text in test_sentences:\n",
    "    translated = translate_banglish(text)\n",
    "    print(f\"Banglish: {text}\")\n",
    "    print(f\"Bengali: {translated}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
